for(col in missing_cols){
data_base <- data_base |> mutate(!!col := "NA")
}
message(str_c(state_message,":: merging"))
with_progress(data_i <- map_merger(data_base,unique(c(aggregation,cols_to_keep))))
st_write(data_i,interm_cache_file,append=FALSE,quiet=TRUE,delete_dsn=TRUE)
}
data_sf <- bind_rows(data_sf,data_i)
rm(data_i)
}
data_sf
data_sf <- data_sf |> st_make_valid()
tryCatch(
data_sf <- fill_holes(data_sf,set_units(smoothing_threshold,"km^2")),
error = function(e) e)
tryCatch(
data_sf <- st_remove_holes(data_sf),
error = function(e) e)
data_sf <- data_sf |> st_make_valid()
data_sf <- data_resolver(data_sf,aggregation,cols_to_keep)
data_sf
aggregation <- as.vector(aggregation)
aggregation <- as.vector(aggregation)
aggregation
aggreg_orig <- aggregation
aggreg_orig
repo_base
aggregation_prefix <- str_extract(aggregation[i],"^[^_]*")
aggregation_suffix <- str_extract(aggregation[i],"[0-9]{4}")
repo_base |>
filter(if_any(c("file_name"), ~ str_detect(.x,aggregation_prefix))) |>
filter(if_any(c("file_name"), ~ str_detect(.x,as.character(aggregation_suffix)))) |>
filter(if_any(c("file_name"), ~ str_detect(.x,"CODE"))) |>
head(1) |>
pull()
aggregation
#new aggregated sum
areas_prop <- list()
for(i in 1:length(aggregation)){
areas_prop[[i]] <- load_aussiemaps_parquet(aggregation[i]) |>
filter(if_any(any_of(c("id")), ~ .x %in% filter_table$id)) |>
collect()
if(!("prop" %in% colnames(areas_prop[[i]]))){
areas_prop[[i]]$prop <- as.numeric(areas_prop[[i]]$area /  areas_prop[[i]]$sum_area)
}
areas_prop[[i]] <- areas_prop[[i]] |>
group_by(across(c("geo_col")))   |>
summarise(across(any_of("prop"), ~ sum(.x)),.groups="drop")
}
aggregation
#new aggregated sum
areas_prop <- list()
areas_prop[[i]] <- load_aussiemaps_parquet(aggregation[i]) |>
filter(if_any(any_of(c("id")), ~ .x %in% filter_table$id)) |>
collect()
load_aussiemaps_parquet(aggregation[i])
aggr_prop  <- aggr_prop[str_detect(aggregation,"CODE")]
aggr_prop
aggr_prop  <- aggregation[str_detect(aggregation,"CODE")]
aggr_prop
aggr_prop  <- aggregation[str_detect(aggregation,"CODE")]
for(i in 1:length(aggr_prop)){
areas_prop[[i]] <- load_aussiemaps_parquet(aggr_prop[i]) |>
filter(if_any(any_of(c("id")), ~ .x %in% filter_table$id)) |>
collect()
if(!("prop" %in% colnames(areas_prop[[i]]))){
areas_prop[[i]]$prop <- as.numeric(areas_prop[[i]]$area /  areas_prop[[i]]$sum_area)
}
areas_prop[[i]] <- areas_prop[[i]] |>
group_by(across(c("geo_col")))   |>
summarise(across(any_of("prop"), ~ sum(.x)),.groups="drop")
}
data_sf$empty <- st_is_empty(data_sf)
data_sf <-  data_sf |>
filter(if_any(any_of(c("empty")), ~ .x==FALSE))|>
select(-any_of(c("empty")))
data_sf
merged_col  <- filter_table |>
mutate(Year=year) |>
select(any_of(c(aggregation,cols_to_merge))) |>
distinct()                                       |>
group_by(across(any_of(c(aggregation))))          |>
reframe(across(any_of(c(aggregation,cols_to_merge)), ~ merge_distinct(.x)))
merged_col
suppressMessages(suppressWarnings(data_sf |>
left_join(merged_col,by=aggregation) |>
relocate(any_of(c("geom","geometry")),.after=last_col())))
data_sf <- suppressMessages(suppressWarnings(data_sf |>
left_join(merged_col,by=aggregation) |>
relocate(any_of(c("geom","geometry")),.after=last_col())))
data_sf
simplification_factor
data_sf |> select(contains("\\.\{d}$"))
data_sf |> select(matches("\\.\{d}$"))
data_sf |> select(matches("\\.[0-9]$"))
data_sf |> select(-matches("\\.[0-9]$"))
?seq_along
source("~/GitHub/aussiemaps/R/get_map.R", echo=TRUE)
library(aussiemaps)
library(aussiemaps)
library(aussiemaps)
library(aussiemaps)
library(aussiemaps)
aussiemaps_file <-"2021_Victoria"
file_names <- load_aussiemaps(aussiemaps_file)
library(piggyback)
librart(arrow)
library(stringr)
library(zip)
librart(fs)
library(fs)
#############################################################
### Internal functions ####
#############################################################
## Based on https://github.com/walkerke/aussiemaps/blob/master/R/helpers.R , released under MIT licence.
#' Helper function to download data from github release
#'
#' @importFrom  piggyback pb_download_url
#' @importFrom  arrow open_dataset
#' @importFrom  stringr str_remove str_c str_detect
#' @importFrom utils download.file
#' @importFrom  zip unzip
#' @importFrom fs path
#' @param aussiemaps_file name of the file to download.
#' @return data frame or parquet binding
#' @noRd
load_aussiemaps <- function(aussiemaps_file) {
cache_files <- data_maps_info()$path
cache_dir   <- find_maps_cache()
file_detect <- any(str_detect(cache_files,aussiemaps_file))
if(!file_detect) {
files <- get_repo_files()$file_name
files <- files[str_detect(files,aussiemaps_file)]
for(filename in files){
#filename <- file
url  <- pb_download_url(filename,
repo = "carlosyanez/aussiemaps",
tag = "data")
file_path <- path(cache_dir,filename)
download.file(url,path(cache_dir,filename))
unzip(file_path,exdir = cache_dir)
file.remove(file_path)
}
}
cache_files <- data_maps_info()$path
file_path <- cache_files[str_detect(cache_files,aussiemaps_file)]
return(file_path)
}
library(sf)
libvrart(stringr)
library(stringr)
library(dplyr)
library(tidyselect)
file_names <- load_aussiemaps(aussiemaps_file)
file_names
file_names <- file_names[str_detect(file_names,"intermediate",TRUE)]
file_names
library(aussiemaps)
file_names <- load_aussiemaps(aussiemaps_file)
file_names <- file_names[str_detect(file_names,"intermediate",TRUE)]
data <- NULL
file_name <- file_names
temp_gpkg <- path(find_maps_cache(),"temp.gpkg")
file_copy(file_name,temp_gpkg,overwrite=TRUE)
data_layer <- st_layers(file_name)$name[1]
data_layer
filter_ids
filter_table <- list_structure(year=2021)
filter_ids = filter_table
st_write(filter_ids,temp_gpkg,layer="id",append=TRUE,quiet=TRUE)
query_text <- str_c("SELECT * FROM '",data_layer,"' WHERE id IN (SELECT id FROM id)")
st_read(temp_gpkg,query=query_text,quiet=TRUE)
data_i <- st_read(temp_gpkg,query=query_text,quiet=TRUE) |>
mutate(across(where(is.character), ~ str_squish(.x))) |>
mutate(across(where(is.character), ~ str_remove_all(.x, "[^A-z|0-9|[:punct:]|\\s]")))
data_i
aussiemaps::data_maps_info()
a<-aussiemaps::data_maps_info()
View(a)
aussiemaps::data_maps_delete(regexp="intermediate")
filter_table=list_structure(2021,list(STATE_NAME_2021="South Australia|Western"))
year=2021
aggregation=c("SA2_NAME_2021","SA2_CODE_2021")
simplification_factor=0.2
new_crs = NULL
fill_holes = TRUE
smoothing_threshold=4
cache_intermediates=FALSE
message_string=""
equals <- st_equals(data_base)
equals[[5813]]
d<-aussiemap::get_map(filter_table,
year,
aggregation,
simplification_factor,
new_crs,
fill_holes,
smoothing_threshold,
cache_intermediates,
message_string)
remotes::install_github("carlosyanez/aussiemaps")
filter_table=list_structure(2021,list(STATE_NAME_2021="South Australia|Western"))
year=2021
aggregation=c("SA2_NAME_2021","SA2_CODE_2021")
simplification_factor=0.2
new_crs = NULL
fill_holes = TRUE
smoothing_threshold=4
cache_intermediates=FALSE
message_string=""
d<-aussiemaps::get_map(filter_table,
year,
aggregation,
simplification_factor,
new_crs,
fill_holes,
smoothing_threshold,
cache_intermediates,
message_string)
filter_table=aussiemaps::list_structure(2021,list(STATE_NAME_2021="South Australia|Western"))
d<-aussiemaps::get_map(filter_table,
year,
aggregation,
simplification_factor,
new_crs,
fill_holes,
smoothing_threshold,
cache_intermediates,
message_string)
d<-aussiemaps::get_map(filfilter_table=ter_table,
year=year,
aggregation=aggregation,
simplification_factor=simplification_factor,
new_crs=new_crs,
fill_holes=fill_holes,
smoothing_threshold=smoothing_threshold,
cache_intermediates=cache_intermediates,
message_string=message_string)
filter_table=aussiemaps::list_structure(2021,list(STATE_NAME_2021="South Australia|Western"))
d<-aussiemaps::get_map(filfilter_table=ter_table,
year=year,
aggregation=aggregation,
simplification_factor=simplification_factor,
new_crs=new_crs,
fill_holes=fill_holes,
smoothing_threshold=smoothing_threshold,
cache_intermediates=cache_intermediates,
message_string=message_string)
d<-aussiemaps::get_map(filter_table=ter_table,
year=year,
aggregation=aggregation,
simplification_factor=simplification_factor,
new_crs=new_crs,
fill_holes=fill_holes,
smoothing_threshold=smoothing_threshold,
cache_intermediates=cache_intermediates,
message_string=message_string)
d<-aussiemaps::get_map(filter_table=filter_table,
year=year,
aggregation=aggregation,
simplification_factor=simplification_factor,
new_crs=new_crs,
fill_holes=fill_holes,
smoothing_threshold=smoothing_threshold,
cache_intermediates=cache_intermediates,
message_string=message_string)
library(aussiemaps)
d<-aussiemaps::get_map(filter_table=filter_table,
year=year,
aggregation=aggregation,
simplification_factor=simplification_factor,
new_crs=new_crs,
fill_holes=fill_holes,
smoothing_threshold=smoothing_threshold,
cache_intermediates=cache_intermediates,
message_string=message_string)
d |>
leaflet() |>
addTiles() |>
addPolygons(fillColor = "green")
library(leaflet)
d |>
leaflet() |>
addTiles() |>
addPolygons(fillColor = "green")
#############################################################
### Internal functions ####
#############################################################
## Based on https://github.com/walkerke/aussiemaps/blob/master/R/helpers.R , released under MIT licence.
#' Helper function to download data from github release
#'
#' @importFrom  piggyback pb_download_url
#' @importFrom  arrow open_dataset
#' @importFrom  stringr str_remove str_c str_detect
#' @importFrom utils download.file
#' @importFrom  zip unzip
#' @importFrom fs path
#' @param aussiemaps_file name of the file to download.
#' @return data frame or parquet binding
#' @noRd
load_aussiemaps <- function(aussiemaps_file) {
cache_files <- data_maps_info()$path
cache_dir   <- find_maps_cache()
file_detect <- any(str_detect(cache_files,aussiemaps_file))
if(!file_detect) {
files <- get_repo_files()$file_name
files <- files[str_detect(files,aussiemaps_file)]
for(filename in files){
#filename <- file
url  <- pb_download_url(filename,
repo = "carlosyanez/aussiemaps",
tag = "data")
file_path <- path(cache_dir,filename)
download.file(url,path(cache_dir,filename))
unzip(file_path,exdir = cache_dir)
file.remove(file_path)
}
}
cache_files <- data_maps_info()$path
file_path <- cache_files[str_detect(cache_files,aussiemaps_file)]
return(file_path)
}
#' Helper function to import gpkg data
#'
#' @importFrom arrow open_dataset
#' @importFrom dplyr bind_rows
#' @param aussiemaps_file name of the file to download.
#' @return sf parquet binding
#' @noRd
load_aussiemaps_parquet <- function(aussiemaps_file){
file_name <- load_aussiemaps(aussiemaps_file)
data <- NULL
for(file in file_name){
data_i <- open_dataset(file,format="parquet")
if(is.null(data)){
data <- data_i
}else{
data <- bind_rows(data,data_i)
}
}
return(data)
}
#' Helper function to import gpkg data
#'
#' @importFrom fs path file_copy
#' @importFrom sf st_write st_read st_layers
#' @importFrom stringr str_c str_remove_all str_squish str_detect
#' @importFrom dplyr mutate across bind_rows
#' @importFrom tidyselect where
#' @param aussiemaps_file name of the file to download.
#' @param filter_ids data frame with ids to filter (id column)
#' @return sf data frame
#' @noRd
load_aussiemaps_gpkg <- function(aussiemaps_file,filter_ids=NULL){
file_names <- load_aussiemaps(aussiemaps_file)
file_names <- file_names[str_detect(file_names,"intermediate",TRUE)]
data <- NULL
for(file_name in file_names){
temp_gpkg <- path(find_maps_cache(),"temp.gpkg")
file_copy(file_name,temp_gpkg,overwrite=TRUE)
data_layer <- st_layers(file_name)$name[1]
if(!is.null(filter_ids)){
st_write(filter_ids,temp_gpkg,layer="id",append=TRUE,quiet=TRUE)
query_text <- str_c("SELECT * FROM '",data_layer,"' WHERE id IN (SELECT id FROM id)")
}else{
query_text <- str_c("SELECT * FROM '",data_layer,"'")
}
data_i <- st_read(temp_gpkg,query=query_text,quiet=TRUE) |>
mutate(across(where(is.character), ~ str_squish(.x))) |>
mutate(across(where(is.character), ~ str_remove_all(.x, "[^A-z|0-9|[:punct:]|\\s]")))
if(is.null(data)){
data <- data_i
}else{
data <- bind_rows(data,data_i)
}
}
return(data)
}
#' Update list of files in repo
#' @importFrom piggyback pb_list
#' @importFrom arrow write_parquet
#' @importFrom fs path file_exists file_info
#' @importFrom lubridate now interval days
#' @noRd
get_repo_files <- function(){
cache_dir <-  find_maps_cache()
local_repo <- path(cache_dir,"repo.parquet")
if(file_exists(local_repo)){
creation <- file_info(local_repo)$birth_time
now <- now()
age <- interval(creation,now)/days(1)
if(age>1){
repo      <- pb_list("carlosyanez/aussiemaps")
write_parquet(repo,path(cache_dir,"repo.parquet"))
}else{
repo <- read_parquet(path(cache_dir,"repo.parquet"))
}
}else{
repo      <- pb_list("carlosyanez/aussiemaps")
write_parquet(repo,path(cache_dir,"repo.parquet"))
}
return(repo)
}
library(sf)
library(fs)
library(tidyverse)
library(aussiemaps)
library(piggyback)
nt <- load_aussiemaps_gpkg("2021_Northern.Territory")
nt
get_map(filtes=list(STATE_NAME_2021="Northern"),year=2021,aggregation = "CED_NAME_2021")
get_map(filters=list(STATE_NAME_2021="Northern"),year=2021,aggregation = "CED_NAME_2021")
get_map(filters=list(STATE_NAME_2021="Territory"),year=2021,aggregation = "CED_NAME_2021")
get_map(filters=list(STATE_NAME_2021="Territor"),year=2021,aggregation = "CED_NAME_2021")
d<- get_map(filters=list(STATE_NAME_2021="Territor"),year=2021,aggregation = "CED_NAME_2021")
d<- get_map(filters=list(STATE_NAME_2021="Territor"),year=2021,aggregation = "CED_NAME_2021",cache_intermediates = FALSE)
library(leaflet)
d |>
leaflet()|>
addTiles() |>
addPolygons()
nt <- load_aussiemaps_gpkg("2021_Victoria")
nt |> filter(is.na(SAL_CODE_2021))
nt |> filter(is.na(SAL_NAME_2021))
d<- get_map(filters=list(STATE_NAME_2021="Territory"),year=2021,aggregation = "CED_NAME_2021")
library(aussiemaps)
d<- get_map(filters=list(STATE_NAME_2021="Territory"),year=2021,aggregation = "CED_NAME_2021")
e <- get_map(filters=list(STATE_NAME_2021="Other"),year=2021,aggregation = "CED_NAME_2021")
d
d$CED_NAME_2021
e$CED_NAME_2021
d <- bind_rows(d,e)
library(tidyverse)
d <- bind_rows(d,e)
data_sf <- d
aggregation <- "CED_NAME_2021"
# aggregation
dups <- data_sf |> st_drop_geometry() |>
count(accros(any_of(aggregation))) |>
filter(if_any(any_of(c("n")), ~.x>1))
library(sf)
# aggregation
dups <- data_sf |> st_drop_geometry() |>
count(accros(any_of(aggregation))) |>
filter(if_any(any_of(c("n")), ~.x>1))
# aggregation
dups <- data_sf |> st_drop_geometry() |>
count(accross(any_of(aggregation))) |>
filter(if_any(any_of(c("n")), ~.x>1))
# aggregation
dups <- data_sf |> st_drop_geometry() |>
count(across(any_of(aggregation))) |>
filter(if_any(any_of(c("n")), ~.x>1))
dups
# aggregation
dups <- data_sf |> st_drop_geometry() |>
count(across(any_of(aggregation))) |>
filter(if_any(any_of(c("n")), ~.x>1))
ndups <- nrow(dups)
ndups
aggr_codes <- aggregation[str_detect(aggregation,"CODE")]
aggr_codes
aggregation <- "CED_CODE_21"
dups_names <- dups |> distinct(any_of(aggregation)) |> mutate(dummy=1)
dups_names <- dups |> select(any_of(aggregation)) |> distinct() |> mutate(dummy=1)
dups_names
aggregation
aggregation <- "CED_NAME_2021"
dups_names <- dups |> select(any_of(aggregation)) |> distinct() |> mutate(dummy=1)
dups_names
mergers <- data_sf |>
left_join(dups_names,by=aggregation) |>
filter(if_any(any_of(c("dummy")), ~.x==1)) |>
select(-any_of(c("dummy")))
mergers
library(aussiemaps)
d<- get_map(filters=list(STATE_NAME_2021="Territor"),year=2021,aggregation = "CED_NAME_2021")
d |>
leaflet()|>
addTiles() |>
addPolygons()
library(leaflet)
d |>
leaflet()|>
addTiles() |>
addPolygons()
library(aussiemaps)
library(sf)
library(fs)
library(tidyverse)
library(aussiemaps)
library(piggyback)
library(leaflet)
d<- get_map(filters=list(STATE_NAME_2021="Territor"),year=2021,aggregation = "CED_NAME_2021")
d
d |>
leaflet()|>
addTiles() |>
addPolygons()
library(aussiemaps)
d<- get_map(filters=list(STATE_NAME_2021="Territor"),year=2021,aggregation = "CED_NAME_2021",use_cache = FALSE)
View(d)
colnames(aussiemaps::list_structure(2021))
library(aussiemaps)
d<- get_map(filters=list(STATE_NAME_2021="Territor"),year=2021,aggregation = "CED_NAME_2021",use_cache = FALSE)
d<- get_map(filters=list(STATE_NAME_2021="Territor"),year=2021,aggregation = "CED_NAME_2021",use_cache = FALSE)
library(aussiemaps)
d<- get_map(filters=list(STATE_NAME_2021="Territor"),year=2021,aggregation = "CED_NAME_2021",use_cache = FALSE)
d |>
leaflet()|>
addTiles() |>
addPolygons()
