data_sf |>
filter(if_any(any_of(c("empty")), ~ .x==FALSE))|>
select(-any_of(c("empty")))
data_sf <-  data_sf |>
filter(if_any(any_of(c("empty")), ~ .x==FALSE))|>
select(-any_of(c("empty")))
merged_col  <- filter_table |>
mutate(Year=year) |>
select(any_of(c(aggregation,cols_to_merge))) |>
distinct()                                       |>
group_by(across(any_of(c(aggregation))))          |>
reframe(across(any_of(cols_to_merge), ~ merge_distinct(.x)))
merge_distinct <- function(x){
x <- unique(x)
x <- sort(x)
str_flatten_comma(x)
}
merged_col  <- filter_table |>
mutate(Year=year) |>
select(any_of(c(aggregation,cols_to_merge))) |>
distinct()                                       |>
group_by(across(any_of(c(aggregation))))          |>
reframe(across(any_of(cols_to_merge), ~ merge_distinct(.x)))
merged_col
data_sf
merged_col
colnames(merged_col)
colnames(data_sf)
aggregation
filter_table |>
mutate(Year=year) |>
select(any_of(c(aggregation,cols_to_merge)))
filter_table |>
mutate(Year=year) |>
select(any_of(c(aggregation,cols_to_merge))) |>
distinct()                                       |>
group_by(across(any_of(c(aggregation))))
filter_table |>
mutate(Year=year) |>
select(any_of(c(aggregation,cols_to_merge))) |>
distinct()                                       |>
group_by(across(any_of(c(aggregation))))          |>
reframe(across(any_of(c(aggregation,cols_to_merge)), ~ merge_distinct(.x)))
colnames(data_sf)
merged_col  <- filter_table |>
mutate(Year=year) |>
select(any_of(c(aggregation,cols_to_merge))) |>
distinct()                                       |>
group_by(across(any_of(c(aggregation))))          |>
reframe(across(any_of(c(aggregation,cols_to_merge)), ~ merge_distinct(.x)))
suppressMessages(suppressWarnings(data_sf |>
left_join(merged_col,by=aggregation) |>
relocate(any_of(c("geom","geometry")),.after=last_col())))
merged_col  <- filter_table |>
mutate(Year=year) |>
select(any_of(c(aggregation,cols_to_merge))) |>
distinct()                                       |>
group_by(across(any_of(c(aggregation))))          |>
reframe(across(any_of(c(aggregation,cols_to_merge)), ~ merge_distinct(.x)))
merged_col
cols_to_keep
cols_to_merge
aggregation
cols_to_merge
agr_names <- aggregation[str_detect(aggregation,"NAME")]
aggr_names <- aggregation[str_detect(aggregation,"NAME")]
aggr_names
aggregation
aggregation <- "LGA_NAME_2021"
<-
aggr_names <- aggregation[str_detect(aggregation,"NAME")]
aggr_names
aggr_codes <- str_replace_all(aggr_names,"NAME","CODE")
aggr_codes
aggregation <- unique(c(aggregation,aggr_codes))
aggregation
aggregation <- aggregation[1]
aggr_names <- aggregation[str_detect(aggregation,"NAME")]
aggr_names
aggregation
aggregation <- "GA_CODE_2021"
aggregation <- "LGA_CODE_2021"
aggr_names <- aggregation[str_detect(aggregation,"NAME")]
aggr_names
aggr_codes <- str_replace_all(aggr_names,"NAME","CODE")
aggr_codes
aggregation <- unique(c(aggregation,aggr_codes))
aggregation
aggregation <- "LGA_NAME_2021"
aggr_names <- aggregation[str_detect(aggregation,"NAME")]
aggr_codes <- str_replace_all(aggr_names,"NAME","CODE")
aggregation <- unique(c(aggregation,aggr_codes))
#just in case, delete any zip files from cache (from aborted reads)
zip_files <- dir_ls(cache_dir,regexp = "zip$")
file_delete(zip_files)
file_regex <- str_c(year,"_[A-Z]{1}")
repo_base <- get_repo_files() |>
mutate(across(any_of(c("file_name")), ~ str_remove_all(.x,"\\.zip"))) |>
select(any_of("file_name"))
repo      <- repo_base |>
filter(if_any(c("file_name"), ~ str_detect(.x,file_regex)))   |>
mutate(across(any_of(c("file_name")), ~ str_remove_all(.x,"\\.[0-9]$")))     |>
distinct() |>
pull()
state_col <- colnames(filter_table)[str_detect(colnames(filter_table),"STATE|STE")]
state_col <- state_col[str_detect(state_col,"NAME")]
required_states <- filter_table |>
select(all_of(c(state_col))) |>
distinct() |>
mutate(across(all_of(as.vector(state_col)), ~ str_replace_all(.x," ","\\."))) |>
mutate(across(all_of(as.vector(state_col)), ~ str_c(year,"_",.x))) |>
filter(if_any(all_of(as.vector(state_col)), ~ .x %in% repo))           |>
pull()
cols_to_keep <-filter_table |>
mutate(across(everything(), as.character)) |>
select(-any_of(c("id","area","Year"))) |>
pivot_longer(-any_of(aggregation),values_to = "value",names_to = "geo_unit") |>
distinct() |>
group_by(across(all_of(c(aggregation,"geo_unit")))) |>
summarise(n=n(),.groups="drop") |>
group_by(across(all_of(c("geo_unit")))) |>
summarise(n=mean(n),.groups="drop") |>
filter(if_any(c("n"), ~ .x==1)) |>
select(any_of("geo_unit")) |>
distinct() |>
pull()
cols_to_keep <- sort(unique(c(cols_to_keep,"Year")))
cols_to_keep <- cols_to_keep[str_detect(cols_to_keep,"AREA_ALBERS_SQKM",TRUE)]
cols_to_keep
cols_to_merge <- colnames(filter_table)
cols_to_merge <- cols_to_merge[!(cols_to_merge %in% c(cols_to_keep,aggregation,"id","area"))]
cols_to_merge <- cols_to_merge[str_detect(cols_to_merge,"AREA_ALBERS_SQKM",TRUE)]
cols_to_merge
data_sf <- NULL
for(repo_i in required_states){
state_message <- str_c(message_string,":: ",repo_i," (",which(repo_i==required_states),"/",length(required_states),")")
message(state_message)
filter_table_hash <- digest(filter_table,"xxhash32",seed=1234)
aggregation_hash <- digest(aggregation,"xxhash32",seed=1234)
interm_cache_file <- path(find_maps_cache(),
str_c("intermediate_",year,"_",repo_i,"_",digest(str_c(repo_i,filter_table_hash,aggregation_hash,sep="-"),
"xxhash32",seed=1234)),
ext="gpkg")
if(file_exists(interm_cache_file) & cache_intermediates){
message(str_c(state_message,":: reading from intermediate cache"))
data_i <- st_read(interm_cache_file,quiet=TRUE)
}else{
message(str_c(state_message,":: normalising"))
data_base <- suppressMessages(suppressWarnings(load_aussiemaps_gpkg(repo_i,filter_table)))
data_base <- data_base |>
mutate(Year=year) |>
mutate(across(where(is.character), ~str_squish(.x))) |>
mutate(across(where(is.character), ~ str_remove_all(.x, "[^A-z|0-9|[:punct:]|\\s]"))) |>
mutate(across(any_of(c("id")), as.character)) |>
st_make_valid()
cols_i <- colnames(data_base)
cols_struct <- colnames(filter_table)
missing_cols <- cols_struct[!(cols_struct %in% cols_i)]
for(col in missing_cols){
data_base <- data_base |> mutate(!!col := "NA")
}
message(str_c(state_message,":: merging"))
with_progress(data_i <- map_merger(data_base,unique(c(aggregation,cols_to_keep))))
st_write(data_i,interm_cache_file,append=FALSE,quiet=TRUE,delete_dsn=TRUE)
}
data_sf <- bind_rows(data_sf,data_i)
rm(data_i)
}
for(repo_i in required_states){
state_message <- str_c(message_string,":: ",repo_i," (",which(repo_i==required_states),"/",length(required_states),")")
message(state_message)
filter_table_hash <- digest(filter_table,"xxhash32",seed=1234)
aggregation_hash <- digest(aggregation,"xxhash32",seed=1234)
interm_cache_file <- path(find_maps_cache(),
str_c("intermediate_",year,"_",repo_i,"_",digest(str_c(repo_i,filter_table_hash,aggregation_hash,sep="-"),
"xxhash32",seed=1234)),
ext="gpkg")
if(file_exists(interm_cache_file) & cache_intermediates){
message(str_c(state_message,":: reading from intermediate cache"))
data_i <- st_read(interm_cache_file,quiet=TRUE)
}else{
message(str_c(state_message,":: normalising"))
data_base <- suppressMessages(suppressWarnings(load_aussiemaps_gpkg(repo_i,filter_table)))
data_base <- data_base |>
mutate(Year=year) |>
mutate(across(where(is.character), ~str_squish(.x))) |>
mutate(across(where(is.character), ~ str_remove_all(.x, "[^A-z|0-9|[:punct:]|\\s]"))) |>
mutate(across(any_of(c("id")), as.character)) |>
st_make_valid()
cols_i <- colnames(data_base)
cols_struct <- colnames(filter_table)
missing_cols <- cols_struct[!(cols_struct %in% cols_i)]
for(col in missing_cols){
data_base <- data_base |> mutate(!!col := "NA")
}
message(str_c(state_message,":: merging"))
with_progress(data_i <- map_merger(data_base,unique(c(aggregation,cols_to_keep))))
st_write(data_i,interm_cache_file,append=FALSE,quiet=TRUE,delete_dsn=TRUE)
}
data_sf <- bind_rows(data_sf,data_i)
rm(data_i)
}
aussiemaps::data_maps_delete(regexp="Northern")
for(repo_i in required_states){
state_message <- str_c(message_string,":: ",repo_i," (",which(repo_i==required_states),"/",length(required_states),")")
message(state_message)
filter_table_hash <- digest(filter_table,"xxhash32",seed=1234)
aggregation_hash <- digest(aggregation,"xxhash32",seed=1234)
interm_cache_file <- path(find_maps_cache(),
str_c("intermediate_",year,"_",repo_i,"_",digest(str_c(repo_i,filter_table_hash,aggregation_hash,sep="-"),
"xxhash32",seed=1234)),
ext="gpkg")
if(file_exists(interm_cache_file) & cache_intermediates){
message(str_c(state_message,":: reading from intermediate cache"))
data_i <- st_read(interm_cache_file,quiet=TRUE)
}else{
message(str_c(state_message,":: normalising"))
data_base <- suppressMessages(suppressWarnings(load_aussiemaps_gpkg(repo_i,filter_table)))
data_base <- data_base |>
mutate(Year=year) |>
mutate(across(where(is.character), ~str_squish(.x))) |>
mutate(across(where(is.character), ~ str_remove_all(.x, "[^A-z|0-9|[:punct:]|\\s]"))) |>
mutate(across(any_of(c("id")), as.character)) |>
st_make_valid()
cols_i <- colnames(data_base)
cols_struct <- colnames(filter_table)
missing_cols <- cols_struct[!(cols_struct %in% cols_i)]
for(col in missing_cols){
data_base <- data_base |> mutate(!!col := "NA")
}
message(str_c(state_message,":: merging"))
with_progress(data_i <- map_merger(data_base,unique(c(aggregation,cols_to_keep))))
st_write(data_i,interm_cache_file,append=FALSE,quiet=TRUE,delete_dsn=TRUE)
}
data_sf <- bind_rows(data_sf,data_i)
rm(data_i)
}
data_sf
data_sf <- data_sf |> st_make_valid()
tryCatch(
data_sf <- fill_holes(data_sf,set_units(smoothing_threshold,"km^2")),
error = function(e) e)
tryCatch(
data_sf <- st_remove_holes(data_sf),
error = function(e) e)
data_sf <- data_sf |> st_make_valid()
data_sf <- data_resolver(data_sf,aggregation,cols_to_keep)
data_sf
aggregation <- as.vector(aggregation)
aggregation <- as.vector(aggregation)
aggregation
aggreg_orig <- aggregation
aggreg_orig
repo_base
aggregation_prefix <- str_extract(aggregation[i],"^[^_]*")
aggregation_suffix <- str_extract(aggregation[i],"[0-9]{4}")
repo_base |>
filter(if_any(c("file_name"), ~ str_detect(.x,aggregation_prefix))) |>
filter(if_any(c("file_name"), ~ str_detect(.x,as.character(aggregation_suffix)))) |>
filter(if_any(c("file_name"), ~ str_detect(.x,"CODE"))) |>
head(1) |>
pull()
aggregation
#new aggregated sum
areas_prop <- list()
for(i in 1:length(aggregation)){
areas_prop[[i]] <- load_aussiemaps_parquet(aggregation[i]) |>
filter(if_any(any_of(c("id")), ~ .x %in% filter_table$id)) |>
collect()
if(!("prop" %in% colnames(areas_prop[[i]]))){
areas_prop[[i]]$prop <- as.numeric(areas_prop[[i]]$area /  areas_prop[[i]]$sum_area)
}
areas_prop[[i]] <- areas_prop[[i]] |>
group_by(across(c("geo_col")))   |>
summarise(across(any_of("prop"), ~ sum(.x)),.groups="drop")
}
aggregation
#new aggregated sum
areas_prop <- list()
areas_prop[[i]] <- load_aussiemaps_parquet(aggregation[i]) |>
filter(if_any(any_of(c("id")), ~ .x %in% filter_table$id)) |>
collect()
load_aussiemaps_parquet(aggregation[i])
aggr_prop  <- aggr_prop[str_detect(aggregation,"CODE")]
aggr_prop
aggr_prop  <- aggregation[str_detect(aggregation,"CODE")]
aggr_prop
aggr_prop  <- aggregation[str_detect(aggregation,"CODE")]
for(i in 1:length(aggr_prop)){
areas_prop[[i]] <- load_aussiemaps_parquet(aggr_prop[i]) |>
filter(if_any(any_of(c("id")), ~ .x %in% filter_table$id)) |>
collect()
if(!("prop" %in% colnames(areas_prop[[i]]))){
areas_prop[[i]]$prop <- as.numeric(areas_prop[[i]]$area /  areas_prop[[i]]$sum_area)
}
areas_prop[[i]] <- areas_prop[[i]] |>
group_by(across(c("geo_col")))   |>
summarise(across(any_of("prop"), ~ sum(.x)),.groups="drop")
}
data_sf$empty <- st_is_empty(data_sf)
data_sf <-  data_sf |>
filter(if_any(any_of(c("empty")), ~ .x==FALSE))|>
select(-any_of(c("empty")))
data_sf
merged_col  <- filter_table |>
mutate(Year=year) |>
select(any_of(c(aggregation,cols_to_merge))) |>
distinct()                                       |>
group_by(across(any_of(c(aggregation))))          |>
reframe(across(any_of(c(aggregation,cols_to_merge)), ~ merge_distinct(.x)))
merged_col
suppressMessages(suppressWarnings(data_sf |>
left_join(merged_col,by=aggregation) |>
relocate(any_of(c("geom","geometry")),.after=last_col())))
data_sf <- suppressMessages(suppressWarnings(data_sf |>
left_join(merged_col,by=aggregation) |>
relocate(any_of(c("geom","geometry")),.after=last_col())))
data_sf
simplification_factor
data_sf |> select(contains("\\.\{d}$"))
data_sf |> select(matches("\\.\{d}$"))
data_sf |> select(matches("\\.[0-9]$"))
data_sf |> select(-matches("\\.[0-9]$"))
?seq_along
source("~/GitHub/aussiemaps/R/get_map.R", echo=TRUE)
library(aussiemaps)
library(aussiemaps)
library(aussiemaps)
library(aussiemaps)
library(aussiemaps)
aussiemaps_file <-"2021_Victoria"
file_names <- load_aussiemaps(aussiemaps_file)
library(piggyback)
librart(arrow)
library(stringr)
library(zip)
librart(fs)
library(fs)
#############################################################
### Internal functions ####
#############################################################
## Based on https://github.com/walkerke/aussiemaps/blob/master/R/helpers.R , released under MIT licence.
#' Helper function to download data from github release
#'
#' @importFrom  piggyback pb_download_url
#' @importFrom  arrow open_dataset
#' @importFrom  stringr str_remove str_c str_detect
#' @importFrom utils download.file
#' @importFrom  zip unzip
#' @importFrom fs path
#' @param aussiemaps_file name of the file to download.
#' @return data frame or parquet binding
#' @noRd
load_aussiemaps <- function(aussiemaps_file) {
cache_files <- data_maps_info()$path
cache_dir   <- find_maps_cache()
file_detect <- any(str_detect(cache_files,aussiemaps_file))
if(!file_detect) {
files <- get_repo_files()$file_name
files <- files[str_detect(files,aussiemaps_file)]
for(filename in files){
#filename <- file
url  <- pb_download_url(filename,
repo = "carlosyanez/aussiemaps",
tag = "data")
file_path <- path(cache_dir,filename)
download.file(url,path(cache_dir,filename))
unzip(file_path,exdir = cache_dir)
file.remove(file_path)
}
}
cache_files <- data_maps_info()$path
file_path <- cache_files[str_detect(cache_files,aussiemaps_file)]
return(file_path)
}
library(sf)
libvrart(stringr)
library(stringr)
library(dplyr)
library(tidyselect)
file_names <- load_aussiemaps(aussiemaps_file)
file_names
file_names <- file_names[str_detect(file_names,"intermediate",TRUE)]
file_names
library(aussiemaps)
file_names <- load_aussiemaps(aussiemaps_file)
file_names <- file_names[str_detect(file_names,"intermediate",TRUE)]
data <- NULL
file_name <- file_names
temp_gpkg <- path(find_maps_cache(),"temp.gpkg")
file_copy(file_name,temp_gpkg,overwrite=TRUE)
data_layer <- st_layers(file_name)$name[1]
data_layer
filter_ids
filter_table <- list_structure(year=2021)
filter_ids = filter_table
st_write(filter_ids,temp_gpkg,layer="id",append=TRUE,quiet=TRUE)
query_text <- str_c("SELECT * FROM '",data_layer,"' WHERE id IN (SELECT id FROM id)")
st_read(temp_gpkg,query=query_text,quiet=TRUE)
data_i <- st_read(temp_gpkg,query=query_text,quiet=TRUE) |>
mutate(across(where(is.character), ~ str_squish(.x))) |>
mutate(across(where(is.character), ~ str_remove_all(.x, "[^A-z|0-9|[:punct:]|\\s]")))
data_i
aussiemaps::data_maps_info()
a<-aussiemaps::data_maps_info()
View(a)
aussiemaps::data_maps_delete(regexp="intermediate")
filter_table=list_structure(2021,list(STATE_NAME_2021="South Australia|Western"))
year=2021
aggregation=c("SA2_NAME_2021","SA2_CODE_2021")
simplification_factor=0.2
new_crs = NULL
fill_holes = TRUE
smoothing_threshold=4
cache_intermediates=FALSE
message_string=""
equals <- st_equals(data_base)
equals[[5813]]
d<-aussiemap::get_map(filter_table,
year,
aggregation,
simplification_factor,
new_crs,
fill_holes,
smoothing_threshold,
cache_intermediates,
message_string)
remotes::install_github("carlosyanez/aussiemaps")
filter_table=list_structure(2021,list(STATE_NAME_2021="South Australia|Western"))
year=2021
aggregation=c("SA2_NAME_2021","SA2_CODE_2021")
simplification_factor=0.2
new_crs = NULL
fill_holes = TRUE
smoothing_threshold=4
cache_intermediates=FALSE
message_string=""
d<-aussiemaps::get_map(filter_table,
year,
aggregation,
simplification_factor,
new_crs,
fill_holes,
smoothing_threshold,
cache_intermediates,
message_string)
filter_table=aussiemaps::list_structure(2021,list(STATE_NAME_2021="South Australia|Western"))
d<-aussiemaps::get_map(filter_table,
year,
aggregation,
simplification_factor,
new_crs,
fill_holes,
smoothing_threshold,
cache_intermediates,
message_string)
d<-aussiemaps::get_map(filfilter_table=ter_table,
year=year,
aggregation=aggregation,
simplification_factor=simplification_factor,
new_crs=new_crs,
fill_holes=fill_holes,
smoothing_threshold=smoothing_threshold,
cache_intermediates=cache_intermediates,
message_string=message_string)
filter_table=aussiemaps::list_structure(2021,list(STATE_NAME_2021="South Australia|Western"))
d<-aussiemaps::get_map(filfilter_table=ter_table,
year=year,
aggregation=aggregation,
simplification_factor=simplification_factor,
new_crs=new_crs,
fill_holes=fill_holes,
smoothing_threshold=smoothing_threshold,
cache_intermediates=cache_intermediates,
message_string=message_string)
d<-aussiemaps::get_map(filter_table=ter_table,
year=year,
aggregation=aggregation,
simplification_factor=simplification_factor,
new_crs=new_crs,
fill_holes=fill_holes,
smoothing_threshold=smoothing_threshold,
cache_intermediates=cache_intermediates,
message_string=message_string)
d<-aussiemaps::get_map(filter_table=filter_table,
year=year,
aggregation=aggregation,
simplification_factor=simplification_factor,
new_crs=new_crs,
fill_holes=fill_holes,
smoothing_threshold=smoothing_threshold,
cache_intermediates=cache_intermediates,
message_string=message_string)
library(aussiemaps)
d<-aussiemaps::get_map(filter_table=filter_table,
year=year,
aggregation=aggregation,
simplification_factor=simplification_factor,
new_crs=new_crs,
fill_holes=fill_holes,
smoothing_threshold=smoothing_threshold,
cache_intermediates=cache_intermediates,
message_string=message_string)
d |>
leaflet() |>
addTiles() |>
addPolygons(fillColor = "green")
library(leaflet)
d |>
leaflet() |>
addTiles() |>
addPolygons(fillColor = "green")
