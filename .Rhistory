select(-any_of(c("filter_flag"))) |>
left_join(distinct_combos[j,],by=by_cols) |>
filter(if_any(c("filter_flag"), ~ .x==TRUE)) |>
select(-any_of(c("filter_flag"))))
data_j <- distinct_combos[j,] |>
add_column(geom=st_union(data_j)) |>
st_as_sf()
if(is.null(data_i)){
data_i <- data_j
}else{
data_i <- bind_rows(data_i,data_j)
}
p(message = sprintf("%g", x[j]))
}
data_i <- data_i |>select(-any_of(c("filter_flag")))
return(data_i)
}
#' @param df df
#' @param aggregation aggregation
#' @param cols_to_keep cols to keep
#' @importFrom stringr str_c
#' @importFrom dplyr select any_of mutate  filter bind_rows
#' @importFrom sf st_cast st_make_valid st_difference st_covers st_area
#' @importFrom progressr with_progress
#' @description Internal function resolve overlaps
data_resolver <- function(df,aggregation,cols_to_keep){
suppressWarnings(suppressMessages(df_i <- df |> st_cast("POLYGON")))
df_i <- df_i |> st_make_valid()
#df$split_id <- 1:nrow(df_i)
diff_list <-  st_covers(df_i)
l <- c()
for(i in 1:length(diff_list)){
if(length(diff_list[[i]])>1) l <-c(l,i)
}
if(!is.null(l)){
message("Overlapping surfaces found")
for(i in l){
diff <- df_i[i,]
small <- df[diff_list[[i]][diff_list[[i]]!=i],]
for(j in 1:nrow(small)){
diff <- st_difference(diff,small[j,])
diff <- st_make_valid(diff)
diff$area <- st_area(diff)
diff <- diff |> filter(area==max(area))
}
diff <- diff |> select(any_of(colnames(df)))
df_i <- df_i |>
mutate(rn=row_number()) |>
filter(rn!=i)           |>
select(-any_of("rn"))     |>
bind_rows(diff)
}
message(str_c(state_message,":: merging after filling holes"))
with_progress(df <- map_merger(df_i,unique(c(aggregation,cols_to_keep))))
}
return(df)
}
d <- data_resolver(data_sf,aggregation,cols_to_keep)
#' @param df df
#' @param by_cols by_cols
#' @importFrom stringr str_c
#' @importFrom dplyr filter if_any left_join select bind_rows distinct mutate any_of
#' @importFrom sf st_union st_as_sf st_drop_geometry
#' @importFrom tibble add_column
#' @importFrom progressr progressor seq_along
#' @description Internal function to create data_i, with progress
#' @noRd
map_merger <- function(df,by_cols){
x <-  1:nrow(distinct_combos)
p <- progressr::progressor(along =x)
data_i <- NULL
distinct_combos <- df |> st_drop_geometry() |>
select(any_of(by_cols)) |>
distinct() |>
mutate(filter_flag=TRUE)
for (j in seq_along(x)){
#message(str_c(state_message,":: merging ",j," out of ",nrow(distinct_combos)))
data_j <- suppressMessages(df |>
select(-any_of(c("filter_flag"))) |>
left_join(distinct_combos[j,],by=by_cols) |>
filter(if_any(c("filter_flag"), ~ .x==TRUE)) |>
select(-any_of(c("filter_flag"))))
suppressWarnings(suppressMessages(data_j <- distinct_combos[j,] |>
add_column(geom=st_union(data_j)) |>
st_as_sf()))
if(is.null(data_i)){
data_i <- data_j
}else{
data_i <- bind_rows(data_i,data_j)
}
p(message = sprintf("%g", x[j]))
}
data_i <- data_i |>select(-any_of(c("filter_flag")))
return(data_i)
}
d <- data_resolver(data_sf,aggregation,cols_to_keep)
#' @param df df
#' @param aggregation aggregation
#' @param cols_to_keep cols to keep
#' @importFrom stringr str_c
#' @importFrom dplyr select any_of mutate  filter bind_rows
#' @importFrom sf st_cast st_make_valid st_difference st_covers st_area
#' @importFrom progressr with_progress
#' @description Internal function resolve overlaps
data_resolver <- function(df,aggregation,cols_to_keep){
suppressWarnings(suppressMessages(df_i <- df |> st_cast("POLYGON")))
df_i <- df_i |> st_make_valid()
#df$split_id <- 1:nrow(df_i)
suppressWarnings(suppressMessages(diff_list <-  st_covers(df_i)))
l <- c()
for(i in 1:length(diff_list)){
if(length(diff_list[[i]])>1) l <-c(l,i)
}
if(!is.null(l)){
message("Overlapping surfaces found")
for(i in l){
diff <- df_i[i,]
small <- df[diff_list[[i]][diff_list[[i]]!=i],]
for(j in 1:nrow(small)){
suppressWarnings(suppressMessages(diff <- st_difference(diff,small[j,])))
diff <- st_make_valid(diff)
diff$area <- st_area(diff)
diff <- diff |> filter(area==max(area))
}
diff <- diff |> select(any_of(colnames(df)))
df_i <- df_i |>
mutate(rn=row_number()) |>
filter(rn!=i)           |>
select(-any_of("rn"))     |>
bind_rows(diff)
}
message(str_c(state_message,":: merging after filling holes"))
with_progress(df <- map_merger(df_i,unique(c(aggregation,cols_to_keep))))
}
return(df)
}
d <- data_resolver(data_sf,aggregation,cols_to_keep)
data_sf <- data_sf |> st_make_valid()
data_sf <- data_resolver(data_sf,aggregation,cols_to_keep)
aggregation <- as.vector(aggregation)
aggreg_orig <- aggregation
aggreg_orig
aggregation
i<-1
aggregation_prefix <- str_extract(aggregation[i],"^[^_]*")
aggregation_suffix <- str_extract(aggregation[i],"[0-9]{4}")
aggregation_suffix
aggregation[i]  <- repo_base |>
filter(if_any(c("file_name"), ~ str_detect(.x,aggregation_prefix))) |>
filter(if_any(c("file_name"), ~ str_detect(.x,as.character(aggregation_suffix)))) |>
filter(if_any(c("file_name"), ~ str_detect(.x,"CODE"))) |>
head(1) |>
pull()
aggregation
external_territories <- any(str_detect(required_states,"Other"))
external_territories
#new aggregated sum
areas_prop <- list()
areas_prop[[i]] <- load_aussiemaps_parquet(aggregation[i]) |>
filter(if_any(any_of(c("id")), ~ .x %in% filter_table$id)) |>
collect() |>
group_by(across(c("geo_col")))   |>
summarise(across(any_of("prop"), ~ sum(.x)),.groups="drop")
areas_prop
load_aussiemaps_parquet(aggregation[i])
load_aussiemaps_parquet(aggregation[i]) |>
filter(if_any(any_of(c("id")), ~ .x %in% filter_table$id)) |>
collect()
if(!("prop" %in% colnames(areas_prop[[i]]))){
areas_prop[[i]]$prop <-  areas_prop[[i]]$area /  areas_prop[[i]]$sum_area
}
areas_prop[[i]]
areas_prop[[i]] <- load_aussiemaps_parquet(aggregation[i]) |>
filter(if_any(any_of(c("id")), ~ .x %in% filter_table$id)) |>
collect()
if(!("prop" %in% colnames(areas_prop[[i]]))){
areas_prop[[i]]$prop <-  areas_prop[[i]]$area /  areas_prop[[i]]$sum_area
}
areas_prop[[i]]
areas_prop[[i]] |>  group_by(across(c("geo_col")))   |>
summarise(across(any_of("prop"), ~ sum(.x)),.groups="drop")
areas_prop[[i]] <- areas_prop[[i]] |>
group_by(across(c("geo_col")))   |>
summarise(across(any_of("prop"), ~ sum(.x)),.groups="drop")
data_sf$empty <- st_is_empty(data_sf)
filter(if_any(any_of(c("empty")), ~ .x==FALSE))|>
select(-any_of(c("empty")))
data_sf |>
filter(if_any(any_of(c("empty")), ~ .x==FALSE))|>
select(-any_of(c("empty")))
data_sf <-  data_sf |>
filter(if_any(any_of(c("empty")), ~ .x==FALSE))|>
select(-any_of(c("empty")))
merged_col  <- filter_table |>
mutate(Year=year) |>
select(any_of(c(aggregation,cols_to_merge))) |>
distinct()                                       |>
group_by(across(any_of(c(aggregation))))          |>
reframe(across(any_of(cols_to_merge), ~ merge_distinct(.x)))
merge_distinct <- function(x){
x <- unique(x)
x <- sort(x)
str_flatten_comma(x)
}
merged_col  <- filter_table |>
mutate(Year=year) |>
select(any_of(c(aggregation,cols_to_merge))) |>
distinct()                                       |>
group_by(across(any_of(c(aggregation))))          |>
reframe(across(any_of(cols_to_merge), ~ merge_distinct(.x)))
merged_col
data_sf
merged_col
colnames(merged_col)
colnames(data_sf)
aggregation
filter_table |>
mutate(Year=year) |>
select(any_of(c(aggregation,cols_to_merge)))
filter_table |>
mutate(Year=year) |>
select(any_of(c(aggregation,cols_to_merge))) |>
distinct()                                       |>
group_by(across(any_of(c(aggregation))))
filter_table |>
mutate(Year=year) |>
select(any_of(c(aggregation,cols_to_merge))) |>
distinct()                                       |>
group_by(across(any_of(c(aggregation))))          |>
reframe(across(any_of(c(aggregation,cols_to_merge)), ~ merge_distinct(.x)))
colnames(data_sf)
merged_col  <- filter_table |>
mutate(Year=year) |>
select(any_of(c(aggregation,cols_to_merge))) |>
distinct()                                       |>
group_by(across(any_of(c(aggregation))))          |>
reframe(across(any_of(c(aggregation,cols_to_merge)), ~ merge_distinct(.x)))
suppressMessages(suppressWarnings(data_sf |>
left_join(merged_col,by=aggregation) |>
relocate(any_of(c("geom","geometry")),.after=last_col())))
merged_col  <- filter_table |>
mutate(Year=year) |>
select(any_of(c(aggregation,cols_to_merge))) |>
distinct()                                       |>
group_by(across(any_of(c(aggregation))))          |>
reframe(across(any_of(c(aggregation,cols_to_merge)), ~ merge_distinct(.x)))
merged_col
cols_to_keep
cols_to_merge
aggregation
cols_to_merge
agr_names <- aggregation[str_detect(aggregation,"NAME")]
aggr_names <- aggregation[str_detect(aggregation,"NAME")]
aggr_names
aggregation
aggregation <- "LGA_NAME_2021"
<-
aggr_names <- aggregation[str_detect(aggregation,"NAME")]
aggr_names
aggr_codes <- str_replace_all(aggr_names,"NAME","CODE")
aggr_codes
aggregation <- unique(c(aggregation,aggr_codes))
aggregation
aggregation <- aggregation[1]
aggr_names <- aggregation[str_detect(aggregation,"NAME")]
aggr_names
aggregation
aggregation <- "GA_CODE_2021"
aggregation <- "LGA_CODE_2021"
aggr_names <- aggregation[str_detect(aggregation,"NAME")]
aggr_names
aggr_codes <- str_replace_all(aggr_names,"NAME","CODE")
aggr_codes
aggregation <- unique(c(aggregation,aggr_codes))
aggregation
aggregation <- "LGA_NAME_2021"
aggr_names <- aggregation[str_detect(aggregation,"NAME")]
aggr_codes <- str_replace_all(aggr_names,"NAME","CODE")
aggregation <- unique(c(aggregation,aggr_codes))
#just in case, delete any zip files from cache (from aborted reads)
zip_files <- dir_ls(cache_dir,regexp = "zip$")
file_delete(zip_files)
file_regex <- str_c(year,"_[A-Z]{1}")
repo_base <- get_repo_files() |>
mutate(across(any_of(c("file_name")), ~ str_remove_all(.x,"\\.zip"))) |>
select(any_of("file_name"))
repo      <- repo_base |>
filter(if_any(c("file_name"), ~ str_detect(.x,file_regex)))   |>
mutate(across(any_of(c("file_name")), ~ str_remove_all(.x,"\\.[0-9]$")))     |>
distinct() |>
pull()
state_col <- colnames(filter_table)[str_detect(colnames(filter_table),"STATE|STE")]
state_col <- state_col[str_detect(state_col,"NAME")]
required_states <- filter_table |>
select(all_of(c(state_col))) |>
distinct() |>
mutate(across(all_of(as.vector(state_col)), ~ str_replace_all(.x," ","\\."))) |>
mutate(across(all_of(as.vector(state_col)), ~ str_c(year,"_",.x))) |>
filter(if_any(all_of(as.vector(state_col)), ~ .x %in% repo))           |>
pull()
cols_to_keep <-filter_table |>
mutate(across(everything(), as.character)) |>
select(-any_of(c("id","area","Year"))) |>
pivot_longer(-any_of(aggregation),values_to = "value",names_to = "geo_unit") |>
distinct() |>
group_by(across(all_of(c(aggregation,"geo_unit")))) |>
summarise(n=n(),.groups="drop") |>
group_by(across(all_of(c("geo_unit")))) |>
summarise(n=mean(n),.groups="drop") |>
filter(if_any(c("n"), ~ .x==1)) |>
select(any_of("geo_unit")) |>
distinct() |>
pull()
cols_to_keep <- sort(unique(c(cols_to_keep,"Year")))
cols_to_keep <- cols_to_keep[str_detect(cols_to_keep,"AREA_ALBERS_SQKM",TRUE)]
cols_to_keep
cols_to_merge <- colnames(filter_table)
cols_to_merge <- cols_to_merge[!(cols_to_merge %in% c(cols_to_keep,aggregation,"id","area"))]
cols_to_merge <- cols_to_merge[str_detect(cols_to_merge,"AREA_ALBERS_SQKM",TRUE)]
cols_to_merge
data_sf <- NULL
for(repo_i in required_states){
state_message <- str_c(message_string,":: ",repo_i," (",which(repo_i==required_states),"/",length(required_states),")")
message(state_message)
filter_table_hash <- digest(filter_table,"xxhash32",seed=1234)
aggregation_hash <- digest(aggregation,"xxhash32",seed=1234)
interm_cache_file <- path(find_maps_cache(),
str_c("intermediate_",year,"_",repo_i,"_",digest(str_c(repo_i,filter_table_hash,aggregation_hash,sep="-"),
"xxhash32",seed=1234)),
ext="gpkg")
if(file_exists(interm_cache_file) & cache_intermediates){
message(str_c(state_message,":: reading from intermediate cache"))
data_i <- st_read(interm_cache_file,quiet=TRUE)
}else{
message(str_c(state_message,":: normalising"))
data_base <- suppressMessages(suppressWarnings(load_aussiemaps_gpkg(repo_i,filter_table)))
data_base <- data_base |>
mutate(Year=year) |>
mutate(across(where(is.character), ~str_squish(.x))) |>
mutate(across(where(is.character), ~ str_remove_all(.x, "[^A-z|0-9|[:punct:]|\\s]"))) |>
mutate(across(any_of(c("id")), as.character)) |>
st_make_valid()
cols_i <- colnames(data_base)
cols_struct <- colnames(filter_table)
missing_cols <- cols_struct[!(cols_struct %in% cols_i)]
for(col in missing_cols){
data_base <- data_base |> mutate(!!col := "NA")
}
message(str_c(state_message,":: merging"))
with_progress(data_i <- map_merger(data_base,unique(c(aggregation,cols_to_keep))))
st_write(data_i,interm_cache_file,append=FALSE,quiet=TRUE,delete_dsn=TRUE)
}
data_sf <- bind_rows(data_sf,data_i)
rm(data_i)
}
for(repo_i in required_states){
state_message <- str_c(message_string,":: ",repo_i," (",which(repo_i==required_states),"/",length(required_states),")")
message(state_message)
filter_table_hash <- digest(filter_table,"xxhash32",seed=1234)
aggregation_hash <- digest(aggregation,"xxhash32",seed=1234)
interm_cache_file <- path(find_maps_cache(),
str_c("intermediate_",year,"_",repo_i,"_",digest(str_c(repo_i,filter_table_hash,aggregation_hash,sep="-"),
"xxhash32",seed=1234)),
ext="gpkg")
if(file_exists(interm_cache_file) & cache_intermediates){
message(str_c(state_message,":: reading from intermediate cache"))
data_i <- st_read(interm_cache_file,quiet=TRUE)
}else{
message(str_c(state_message,":: normalising"))
data_base <- suppressMessages(suppressWarnings(load_aussiemaps_gpkg(repo_i,filter_table)))
data_base <- data_base |>
mutate(Year=year) |>
mutate(across(where(is.character), ~str_squish(.x))) |>
mutate(across(where(is.character), ~ str_remove_all(.x, "[^A-z|0-9|[:punct:]|\\s]"))) |>
mutate(across(any_of(c("id")), as.character)) |>
st_make_valid()
cols_i <- colnames(data_base)
cols_struct <- colnames(filter_table)
missing_cols <- cols_struct[!(cols_struct %in% cols_i)]
for(col in missing_cols){
data_base <- data_base |> mutate(!!col := "NA")
}
message(str_c(state_message,":: merging"))
with_progress(data_i <- map_merger(data_base,unique(c(aggregation,cols_to_keep))))
st_write(data_i,interm_cache_file,append=FALSE,quiet=TRUE,delete_dsn=TRUE)
}
data_sf <- bind_rows(data_sf,data_i)
rm(data_i)
}
aussiemaps::data_maps_delete(regexp="Northern")
for(repo_i in required_states){
state_message <- str_c(message_string,":: ",repo_i," (",which(repo_i==required_states),"/",length(required_states),")")
message(state_message)
filter_table_hash <- digest(filter_table,"xxhash32",seed=1234)
aggregation_hash <- digest(aggregation,"xxhash32",seed=1234)
interm_cache_file <- path(find_maps_cache(),
str_c("intermediate_",year,"_",repo_i,"_",digest(str_c(repo_i,filter_table_hash,aggregation_hash,sep="-"),
"xxhash32",seed=1234)),
ext="gpkg")
if(file_exists(interm_cache_file) & cache_intermediates){
message(str_c(state_message,":: reading from intermediate cache"))
data_i <- st_read(interm_cache_file,quiet=TRUE)
}else{
message(str_c(state_message,":: normalising"))
data_base <- suppressMessages(suppressWarnings(load_aussiemaps_gpkg(repo_i,filter_table)))
data_base <- data_base |>
mutate(Year=year) |>
mutate(across(where(is.character), ~str_squish(.x))) |>
mutate(across(where(is.character), ~ str_remove_all(.x, "[^A-z|0-9|[:punct:]|\\s]"))) |>
mutate(across(any_of(c("id")), as.character)) |>
st_make_valid()
cols_i <- colnames(data_base)
cols_struct <- colnames(filter_table)
missing_cols <- cols_struct[!(cols_struct %in% cols_i)]
for(col in missing_cols){
data_base <- data_base |> mutate(!!col := "NA")
}
message(str_c(state_message,":: merging"))
with_progress(data_i <- map_merger(data_base,unique(c(aggregation,cols_to_keep))))
st_write(data_i,interm_cache_file,append=FALSE,quiet=TRUE,delete_dsn=TRUE)
}
data_sf <- bind_rows(data_sf,data_i)
rm(data_i)
}
data_sf
data_sf <- data_sf |> st_make_valid()
tryCatch(
data_sf <- fill_holes(data_sf,set_units(smoothing_threshold,"km^2")),
error = function(e) e)
tryCatch(
data_sf <- st_remove_holes(data_sf),
error = function(e) e)
data_sf <- data_sf |> st_make_valid()
data_sf <- data_resolver(data_sf,aggregation,cols_to_keep)
data_sf
aggregation <- as.vector(aggregation)
aggregation <- as.vector(aggregation)
aggregation
aggreg_orig <- aggregation
aggreg_orig
repo_base
aggregation_prefix <- str_extract(aggregation[i],"^[^_]*")
aggregation_suffix <- str_extract(aggregation[i],"[0-9]{4}")
repo_base |>
filter(if_any(c("file_name"), ~ str_detect(.x,aggregation_prefix))) |>
filter(if_any(c("file_name"), ~ str_detect(.x,as.character(aggregation_suffix)))) |>
filter(if_any(c("file_name"), ~ str_detect(.x,"CODE"))) |>
head(1) |>
pull()
aggregation
#new aggregated sum
areas_prop <- list()
for(i in 1:length(aggregation)){
areas_prop[[i]] <- load_aussiemaps_parquet(aggregation[i]) |>
filter(if_any(any_of(c("id")), ~ .x %in% filter_table$id)) |>
collect()
if(!("prop" %in% colnames(areas_prop[[i]]))){
areas_prop[[i]]$prop <- as.numeric(areas_prop[[i]]$area /  areas_prop[[i]]$sum_area)
}
areas_prop[[i]] <- areas_prop[[i]] |>
group_by(across(c("geo_col")))   |>
summarise(across(any_of("prop"), ~ sum(.x)),.groups="drop")
}
aggregation
#new aggregated sum
areas_prop <- list()
areas_prop[[i]] <- load_aussiemaps_parquet(aggregation[i]) |>
filter(if_any(any_of(c("id")), ~ .x %in% filter_table$id)) |>
collect()
load_aussiemaps_parquet(aggregation[i])
aggr_prop  <- aggr_prop[str_detect(aggregation,"CODE")]
aggr_prop
aggr_prop  <- aggregation[str_detect(aggregation,"CODE")]
aggr_prop
aggr_prop  <- aggregation[str_detect(aggregation,"CODE")]
for(i in 1:length(aggr_prop)){
areas_prop[[i]] <- load_aussiemaps_parquet(aggr_prop[i]) |>
filter(if_any(any_of(c("id")), ~ .x %in% filter_table$id)) |>
collect()
if(!("prop" %in% colnames(areas_prop[[i]]))){
areas_prop[[i]]$prop <- as.numeric(areas_prop[[i]]$area /  areas_prop[[i]]$sum_area)
}
areas_prop[[i]] <- areas_prop[[i]] |>
group_by(across(c("geo_col")))   |>
summarise(across(any_of("prop"), ~ sum(.x)),.groups="drop")
}
data_sf$empty <- st_is_empty(data_sf)
data_sf <-  data_sf |>
filter(if_any(any_of(c("empty")), ~ .x==FALSE))|>
select(-any_of(c("empty")))
data_sf
merged_col  <- filter_table |>
mutate(Year=year) |>
select(any_of(c(aggregation,cols_to_merge))) |>
distinct()                                       |>
group_by(across(any_of(c(aggregation))))          |>
reframe(across(any_of(c(aggregation,cols_to_merge)), ~ merge_distinct(.x)))
merged_col
suppressMessages(suppressWarnings(data_sf |>
left_join(merged_col,by=aggregation) |>
relocate(any_of(c("geom","geometry")),.after=last_col())))
data_sf <- suppressMessages(suppressWarnings(data_sf |>
left_join(merged_col,by=aggregation) |>
relocate(any_of(c("geom","geometry")),.after=last_col())))
data_sf
simplification_factor
data_sf |> select(contains("\\.\{d}$"))
data_sf |> select(matches("\\.\{d}$"))
data_sf |> select(matches("\\.[0-9]$"))
data_sf |> select(-matches("\\.[0-9]$"))
?seq_along
source("~/GitHub/aussiemaps/R/get_map.R", echo=TRUE)
